{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Program\\PycharmProjects\\quoraDupli\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 工作路径改为/quoraDupli\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 载入经过预处理的数据\n",
    "df = pd.read_pickle(os.path.join('data', 'questionVector.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次交叉验证... ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adam\n",
    "# 将dataframe打乱顺序\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# 初始化\n",
    "num_df = df.shape[0]\n",
    "X = np.zeros([num_df, 2, 300])  # (404288, 2, 300)\n",
    "Y = np.zeros([num_df])\n",
    "\n",
    "b = [a[None,:] for a in list(df['q1Vector'].values)]\n",
    "q1_feats = np.concatenate(b, axis=0)  # (404288, 300)\n",
    "\n",
    "b = [a[None,:] for a in list(df['q2Vector'].values)]\n",
    "q2_feats = np.concatenate(b, axis=0)\n",
    "\n",
    "X[:,0,:] = q1_feats\n",
    "X[:,1,:] = q2_feats\n",
    "Y = df['is_duplicate'].values\n",
    "\n",
    "del b\n",
    "del q1_feats\n",
    "del q2_feats\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    cv_num = 1\n",
    "    print '第%d次交叉验证开始... ...' % cv_num\n",
    "    cv_num += 1\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_train = Y[train_index]\n",
    "    Y_test = Y[test_index]\n",
    "    \n",
    "    # 特征向量规范\n",
    "    X_train_norm = np.zeros_like(X_train)\n",
    "    d = (np.sum(X_train[:,0,:] ** 2, 1) ** (0.5))\n",
    "    X_train_norm[:,0,:] = (X_train[:,0,:].T / (d + 1e-8)).T\n",
    "    d = (np.sum(X_train[:,1,:] ** 2, 1) ** (0.5))\n",
    "    X_train_norm[:,1,:] = (X_train[:,1,:].T / (d + 1e-8)).T\n",
    "    \n",
    "    X_test_norm = np.zeros_like(X_test)\n",
    "    d = (np.sum(X_test[:,0,:] ** 2, 1) ** (0.5))\n",
    "    X_test_norm[:,0,:] = (X_test[:,0,:].T / (d + 1e-8)).T\n",
    "    d = (np.sum(X_test[:,1,:] ** 2, 1) ** (0.5))\n",
    "    X_test_norm[:,1,:] = (X_test[:,1,:].T / (d + 1e-8)).T\n",
    "    \n",
    "    net = create_network(300)\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    net.compile(loss=contrastive_loss, optimizer=optimizer)\n",
    "    for epoch in range(5):\n",
    "    net.fit([X_train_norm[:,0,:], X_train_norm[:,1,:]], Y_train,\n",
    "            validation_data=([X_test_norm[:,0,:], X_test_norm[:,1,:]], Y_test),\n",
    "            batch_size=128, nb_epoch=1, shuffle=True, )\n",
    "    \n",
    "    # 预测\n",
    "    pred = net.predict([X_test_norm[:,0,:], X_test_norm[:,1,:]], batch_size=128)\n",
    "    te_acc = compute_accuracy(pred, Y_test)\n",
    "    te_f1 = compute_f1(pred, Y_test)\n",
    "    print '测试集上正确率: %0.2f%%' % (100 * te_acc)\n",
    "    print '测试集上F1值: %0.6f' % te_f1\n",
    "    print '第%d次交叉验证完成。' % cv_num\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Lambda, merge, BatchNormalization, Activation, Input, Merge\n",
    "from keras import backend as K\n",
    "\n",
    "def create_network(input_dim):\n",
    "    # 网络初始化\n",
    "    base_network = create_base_network(input_dim)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    \n",
    "    model = Model(input=[input_a, input_b], output=distance)\n",
    "    return model\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    \n",
    "    input = Input(shape=(input_dim, ))\n",
    "    dense1 = Dense(128)(input)\n",
    "    bn1 = BatchNormalization()(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "\n",
    "    dense2 = Dense(128)(relu1)\n",
    "    bn2 = BatchNormalization()(dense2)\n",
    "    res2 = merge([relu1, bn2], mode='sum')\n",
    "    relu2 = Activation('relu')(res2)    \n",
    "\n",
    "    dense3 = Dense(128)(relu2)\n",
    "    bn3 = BatchNormalization()(dense3)\n",
    "    res3 = Merge(mode='sum')([relu2, bn3])\n",
    "    relu3 = Activation('relu')(res3)   \n",
    "    \n",
    "    feats = merge([relu3, relu2, relu1], mode='concat')\n",
    "    bn4 = BatchNormalization()(feats)\n",
    "\n",
    "    model = Model(input=input, output=bn4)\n",
    "\n",
    "    return model\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n",
    "def compute_f1(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Lambda, merge, BatchNormalization, Activation, Input, Merge\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    \n",
    "    input = Input(shape=(input_dim, ))\n",
    "    dense1 = Dense(128)(input)\n",
    "    bn1 = BatchNormalization()(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "\n",
    "    dense2 = Dense(128)(relu1)\n",
    "    bn2 = BatchNormalization()(dense2)\n",
    "    res2 = merge([relu1, bn2], mode='sum')\n",
    "    relu2 = Activation('relu')(res2)    \n",
    "\n",
    "    dense3 = Dense(128)(relu2)\n",
    "    bn3 = BatchNormalization()(dense3)\n",
    "    res3 = Merge(mode='sum')([relu2, bn3])\n",
    "    relu3 = Activation('relu')(res3)   \n",
    "    \n",
    "    feats = merge([relu3, relu2, relu1], mode='concat')\n",
    "    bn4 = BatchNormalization()(feats)\n",
    "\n",
    "    model = Model(input=input, output=bn4)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    '''\n",
    "    Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n",
    "def create_network(input_dim):\n",
    "    # 网络初始化\n",
    "    base_network = create_base_network(input_dim)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    \n",
    "    model = Model(input=[input_a, input_b], output=distance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/home/chenzh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:45: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/software/home/chenzh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/software/home/chenzh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:51: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/software/home/chenzh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:54: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/software/home/chenzh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ba..., inputs=Tensor(\"in...)`\n",
      "/software/home/chenzh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"la..., inputs=[<tf.Tenso...)`\n",
      "/software/home/chenzh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:12: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 21s - loss: 5.4071 - val_loss: 0.2220\n",
      "* Accuracy on test set: 57.60%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1852 - val_loss: 0.1752\n",
      "* Accuracy on test set: 63.85%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1658 - val_loss: 0.1643\n",
      "* Accuracy on test set: 66.40%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 20s - loss: 0.1568 - val_loss: 0.1575\n",
      "* Accuracy on test set: 68.96%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1499 - val_loss: 0.1530\n",
      "* Accuracy on test set: 69.18%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1441 - val_loss: 0.1487\n",
      "* Accuracy on test set: 70.65%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1389 - val_loss: 0.1460\n",
      "* Accuracy on test set: 71.28%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1348 - val_loss: 0.1446\n",
      "* Accuracy on test set: 71.16%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1314 - val_loss: 0.1436\n",
      "* Accuracy on test set: 73.66%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1286 - val_loss: 0.1417\n",
      "* Accuracy on test set: 72.23%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1258 - val_loss: 0.1410\n",
      "* Accuracy on test set: 71.44%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1237 - val_loss: 0.1406\n",
      "* Accuracy on test set: 72.24%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1217 - val_loss: 0.1398\n",
      "* Accuracy on test set: 74.07%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1198 - val_loss: 0.1393\n",
      "* Accuracy on test set: 73.17%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1183 - val_loss: 0.1388\n",
      "* Accuracy on test set: 73.71%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1171 - val_loss: 0.1388\n",
      "* Accuracy on test set: 73.79%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 20s - loss: 0.1157 - val_loss: 0.1390\n",
      "* Accuracy on test set: 73.99%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1146 - val_loss: 0.1385\n",
      "* Accuracy on test set: 74.62%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 20s - loss: 0.1137 - val_loss: 0.1382\n",
      "* Accuracy on test set: 74.04%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1124 - val_loss: 0.1377\n",
      "* Accuracy on test set: 73.75%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1119 - val_loss: 0.1374\n",
      "* Accuracy on test set: 73.99%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1110 - val_loss: 0.1376\n",
      "* Accuracy on test set: 73.93%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1101 - val_loss: 0.1383\n",
      "* Accuracy on test set: 75.02%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1093 - val_loss: 0.1374\n",
      "* Accuracy on test set: 74.14%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1088 - val_loss: 0.1375\n",
      "* Accuracy on test set: 73.39%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1082 - val_loss: 0.1376\n",
      "* Accuracy on test set: 74.16%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1077 - val_loss: 0.1370\n",
      "* Accuracy on test set: 74.68%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1071 - val_loss: 0.1372\n",
      "* Accuracy on test set: 74.30%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1067 - val_loss: 0.1366\n",
      "* Accuracy on test set: 74.48%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1062 - val_loss: 0.1373\n",
      "* Accuracy on test set: 75.03%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1059 - val_loss: 0.1366\n",
      "* Accuracy on test set: 75.08%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1051 - val_loss: 0.1360\n",
      "* Accuracy on test set: 74.24%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1049 - val_loss: 0.1364\n",
      "* Accuracy on test set: 74.00%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1045 - val_loss: 0.1368\n",
      "* Accuracy on test set: 74.96%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1039 - val_loss: 0.1364\n",
      "* Accuracy on test set: 74.54%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1038 - val_loss: 0.1364\n",
      "* Accuracy on test set: 74.77%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1034 - val_loss: 0.1364\n",
      "* Accuracy on test set: 74.88%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1029 - val_loss: 0.1359\n",
      "* Accuracy on test set: 74.59%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1029 - val_loss: 0.1362\n",
      "* Accuracy on test set: 74.77%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 20s - loss: 0.1025 - val_loss: 0.1363\n",
      "* Accuracy on test set: 74.26%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1021 - val_loss: 0.1360\n",
      "* Accuracy on test set: 74.78%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 20s - loss: 0.1018 - val_loss: 0.1363\n",
      "* Accuracy on test set: 74.05%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1014 - val_loss: 0.1360\n",
      "* Accuracy on test set: 74.25%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1013 - val_loss: 0.1361\n",
      "* Accuracy on test set: 75.11%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1010 - val_loss: 0.1360\n",
      "* Accuracy on test set: 74.76%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1009 - val_loss: 0.1366\n",
      "* Accuracy on test set: 74.43%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 20s - loss: 0.1007 - val_loss: 0.1368\n",
      "* Accuracy on test set: 74.87%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1002 - val_loss: 0.1366\n",
      "* Accuracy on test set: 75.01%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.1002 - val_loss: 0.1371\n",
      "* Accuracy on test set: 75.43%\n",
      "Train on 355773 samples, validate on 48515 samples\n",
      "Epoch 1/1\n",
      "355773/355773 [==============================] - 19s - loss: 0.0999 - val_loss: 0.1362\n",
      "* Accuracy on test set: 74.78%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "net = create_network(300)\n",
    "\n",
    "# train\n",
    "#optimizer = SGD(lr=1, momentum=0.8, nesterov=True, decay=0.004)\n",
    "optimizer = Adam(lr=0.001)\n",
    "net.compile(loss=contrastive_loss, optimizer=optimizer)\n",
    "\n",
    "for epoch in range(50):\n",
    "    net.fit([X_train_norm[:,0,:], X_train_norm[:,1,:]], Y_train,\n",
    "          validation_data=([X_test_norm[:,0,:], X_test_norm[:,1,:]], Y_test),\n",
    "          batch_size=128, nb_epoch=1, shuffle=True, )\n",
    "    \n",
    "    # compute final accuracy on training and test sets\n",
    "    pred = net.predict([X_test_norm[:,0,:], X_test_norm[:,1,:]], batch_size=128)\n",
    "    te_acc = compute_accuracy(pred, Y_test)\n",
    "    \n",
    "#    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "    print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
