{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows (question pairs): 404290\n",
      "0    255027\n",
      "1    149263\n",
      "Name: is_duplicate, dtype: int64\n",
      "number of unique questions: 537933\n"
     ]
    }
   ],
   "source": [
    "# 载入数据\n",
    "df = pd.read_csv(\"D:\\Program\\PycharmProjects\\quoraDupli\\data\\quora_duplicate_questions.tsv\",delimiter='\\t')\n",
    "\n",
    "# 统计\n",
    "print(\"number of rows (question pairs): %i\"%(df.shape[0]))\n",
    "print(df['is_duplicate'].value_counts())\n",
    "unique_qids = set(list(df['qid2'].unique()) + list(df['qid1'].unique()))\n",
    "print(\"number of unique questions: %i\" % (len(unique_qids)))\n",
    "\n",
    "# 转化编码，避免NaN\n",
    "df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, )\n",
    "tfidf.fit_transform(questions)\n",
    "\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "del questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('data/2_word2vec_tfidf.pkl'):\n",
    "    df = pd.read_pickle('data/2_word2vec_tfidf.pkl')\n",
    "else:\n",
    "    # exctract word2vec vectors\n",
    "    import spacy\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    vecs1 = []\n",
    "    for qu in tqdm(list(df['question1'])):\n",
    "        doc = nlp(qu) \n",
    "        mean_vec = np.zeros([len(doc), 300])\n",
    "        for word in doc:\n",
    "            # word2vec\n",
    "            vec = word.vector\n",
    "            # fetch df score\n",
    "            try:\n",
    "                idf = word2tfidf[str(word)]\n",
    "            except:\n",
    "                #print word\n",
    "                idf = 0\n",
    "            # compute final vec\n",
    "            mean_vec += vec * idf\n",
    "        mean_vec = mean_vec.mean(axis=0)\n",
    "        vecs1.append(mean_vec)\n",
    "    df['q1_feats'] = list(vecs1)\n",
    "    \n",
    "    vecs2 = []\n",
    "    for qu in tqdm(list(df['question2'])):\n",
    "        doc = nlp(qu) \n",
    "        mean_vec = np.zeros([len(doc), 300])\n",
    "        for word in doc:\n",
    "            # word2vec\n",
    "            vec = word.vector\n",
    "            # fetch df score\n",
    "            try:\n",
    "                idf = word2tfidf[str(word)]\n",
    "            except:\n",
    "                print word\n",
    "                idf = 0\n",
    "            # compute final vec\n",
    "            mean_vec += vec * idf\n",
    "        mean_vec = mean_vec.mean(axis=0)\n",
    "        vecs2.append(mean_vec)\n",
    "    df['q2_feats'] = list(vecs2)\n",
    "\n",
    "    # save features\n",
    "    pd.to_pickle(df, 'data/2_word2vec_tfidf.pkl')\n",
    "\n",
    "##############################################################################\n",
    "# CREATE TRAIN DATA\n",
    "##############################################################################\n",
    "# shuffle df\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# set number of train and test instances\n",
    "num_train = int(df.shape[0] * 0.88)\n",
    "num_test = df.shape[0] - num_train                 \n",
    "print(\"Number of training pairs: %i\"%(num_train))\n",
    "print(\"Number of testing pairs: %i\"%(num_test))\n",
    "\n",
    "# init data data arrays\n",
    "X_train = np.zeros([num_train, 2, 300])\n",
    "X_test  = np.zeros([num_test, 2, 300])\n",
    "Y_train = np.zeros([num_train]) \n",
    "Y_test = np.zeros([num_test])\n",
    "\n",
    "# format data \n",
    "b = [a[None,:] for a in list(df['q1_feats'].values)]\n",
    "q1_feats = np.concatenate(b, axis=0)\n",
    "\n",
    "b = [a[None,:] for a in list(df['q2_feats'].values)]\n",
    "q2_feats = np.concatenate(b, axis=0)\n",
    "\n",
    "# fill data arrays with features\n",
    "X_train[:,0,:] = q1_feats[:num_train]\n",
    "X_train[:,1,:] = q2_feats[:num_train]\n",
    "Y_train = df[:num_train]['is_duplicate'].values\n",
    "            \n",
    "X_test[:,0,:] = q1_feats[num_train:]\n",
    "X_test[:,1,:] = q2_feats[num_train:]\n",
    "Y_test = df[num_train:]['is_duplicate'].values\n",
    "\n",
    "# remove useless variables\n",
    "del b\n",
    "del q1_feats\n",
    "del q2_feats\n",
    "\n",
    "# preprocess data, unit std\n",
    "#X_train_norm = np.zeros_like(X_train)\n",
    "#d = (np.sum(X_train[:,0,:] ** 2, 1) ** (0.5))\n",
    "#X_train_norm[:,0,:] = (X_train[:,0,:].T / (d + 1e-8)).T\n",
    "#d = (np.sum(X_train[:,1,:] ** 2, 1) ** (0.5))\n",
    "#X_train_norm[:,1,:] = (X_train[:,1,:].T / (d + 1e-8)).T\n",
    "#            \n",
    "#            \n",
    "#X_test_norm = np.zeros_like(X_test)\n",
    "#d = (np.sum(X_test[:,0,:] ** 2, 1) ** (0.5))\n",
    "#X_test_norm[:,0,:] = (X_test[:,0,:].T / (d + 1e-8)).T\n",
    "#d = (np.sum(X_test[:,1,:] ** 2, 1) ** (0.5))\n",
    "#X_test_norm[:,1,:] = (X_test[:,1,:].T / (d + 1e-8)).T\n",
    "\n",
    "##############################################################################\n",
    "# TRAIN MODEL\n",
    "# - 2 layers net : 0.67\n",
    "# - 3 layers net + adam : 0.74\n",
    "# - 3 layers resnet (after relu) + adam : 0.78\n",
    "# - 3 layers resnet (before relu) + adam : 0.77\n",
    "# - 3 layers resnet (before relu) + adam + dropout : 0.75\n",
    "# - 3 layers resnet (before relu) + adam + layer concat : 0.79\n",
    "# - 3 layers resnet (before relu) + adam + layer concat + unit_norm : 0.77\n",
    "# - 3 layers resnet (before relu) + adam + unit_norm + cosine_distance : Fail\n",
    "############################################################################## \n",
    "          \n",
    "# create model\n",
    "from siamese import *\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "net = create_network(300)\n",
    "\n",
    "# train\n",
    "#optimizer = SGD(lr=1, momentum=0.8, nesterov=True, decay=0.004)\n",
    "optimizer = Adam(lr=0.001)\n",
    "net.compile(loss=contrastive_loss, optimizer=optimizer)\n",
    "\n",
    "for epoch in range(50):\n",
    "    net.fit([X_train_norm[:,0,:], X_train_norm[:,1,:]], Y_train,\n",
    "          validation_data=([X_test_norm[:,0,:], X_test_norm[:,1,:]], Y_test),\n",
    "          batch_size=128, nb_epoch=1, shuffle=True, )\n",
    "    \n",
    "    # compute final accuracy on training and test sets\n",
    "    pred = net.predict([X_test_norm[:,0,:], X_test_norm[:,1,:]], batch_size=128)\n",
    "    te_acc = compute_accuracy(pred, Y_test)\n",
    "    \n",
    "#    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "    print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
