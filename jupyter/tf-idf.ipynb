{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "",
  "signature": "sha256:b775c5393072ba5d0b7ae13a1b9eb8e3495b95ddb83d73f22ad4a5975d128620"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os  \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import gensim\n",
      "from tqdm import tqdm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \u8f7d\u5165\u6570\u636e\n",
      "# df = pd.read_csv(\"D:\\Program\\PycharmProjects\\quoraDupli\\data\\quora_duplicate_questions.tsv\",delimiter='\\t')\n",
      "df = pd.read_csv(\"/home/zydar/software/data/PycharmProjects/quoraDupli/data/quora_duplicate_questions.tsv\",delimiter='\\t')\n",
      "# \u7edf\u8ba1\n",
      "print(\"number of rows (question pairs): %i\"%(df.shape[0]))\n",
      "print(df['is_duplicate'].value_counts())\n",
      "unique_qids = set(list(df['qid2'].unique()) + list(df['qid1'].unique()))\n",
      "print(\"number of unique questions: %i\" % (len(unique_qids)))\n",
      "\n",
      "# \u8f6c\u5316\u7f16\u7801\uff0c\u907f\u514dNaN\n",
      "df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
      "df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of rows (question pairs): 404290\n",
        "0    255027\n",
        "1    149263\n",
        "Name: is_duplicate, dtype: int64\n",
        "number of unique questions: 537933"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "TFIDF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "questions = list(df['question1']) + list(df['question2'])\n",
      "\n",
      "tfidf = TfidfVectorizer(lowercase=False, )\n",
      "tfidf.fit_transform(questions)\n",
      "\n",
      "# dict key:word and value:tf-idf score\n",
      "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
      "del questions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "if os.path.exists('data/2_word2vec_tfidf.pkl'):\n",
      "    df = pd.read_pickle('data/2_word2vec_tfidf.pkl')\n",
      "else:\n",
      "    # exctract word2vec vectors\n",
      "    import spacy\n",
      "    nlp = spacy.load('en')\n",
      "    \n",
      "    vecs1 = []\n",
      "    for qu in tqdm(list(df['question1'])):\n",
      "        doc = nlp(qu) \n",
      "        mean_vec = np.zeros([len(doc), 300])\n",
      "        for word in doc:\n",
      "            # word2vec\n",
      "            vec = word.vector\n",
      "            # fetch df score\n",
      "            try:\n",
      "                idf = word2tfidf[str(word)]\n",
      "            except:\n",
      "                #print word\n",
      "                idf = 0\n",
      "            # compute final vec\n",
      "            mean_vec += vec * idf\n",
      "        mean_vec = mean_vec.mean(axis=0)\n",
      "        vecs1.append(mean_vec)\n",
      "    df['q1_feats'] = list(vecs1)\n",
      "    \n",
      "    vecs2 = []\n",
      "    for qu in tqdm(list(df['question2'])):\n",
      "        doc = nlp(qu) \n",
      "        mean_vec = np.zeros([len(doc), 300])\n",
      "        for word in doc:\n",
      "            # word2vec\n",
      "            vec = word.vector\n",
      "            # fetch df score\n",
      "            try:\n",
      "                idf = word2tfidf[str(word)]\n",
      "            except:\n",
      "                print word\n",
      "                idf = 0\n",
      "            # compute final vec\n",
      "            mean_vec += vec * idf\n",
      "        mean_vec = mean_vec.mean(axis=0)\n",
      "        vecs2.append(mean_vec)\n",
      "    df['q2_feats'] = list(vecs2)\n",
      "\n",
      "    # save features\n",
      "    pd.to_pickle(df, 'data/2_word2vec_tfidf.pkl')\n",
      "\n",
      "##############################################################################\n",
      "# CREATE TRAIN DATA\n",
      "##############################################################################\n",
      "# shuffle df\n",
      "df = df.reindex(np.random.permutation(df.index))\n",
      "\n",
      "# set number of train and test instances\n",
      "num_train = int(df.shape[0] * 0.88)\n",
      "num_test = df.shape[0] - num_train                 \n",
      "print(\"Number of training pairs: %i\"%(num_train))\n",
      "print(\"Number of testing pairs: %i\"%(num_test))\n",
      "\n",
      "# init data data arrays\n",
      "X_train = np.zeros([num_train, 2, 300])\n",
      "X_test  = np.zeros([num_test, 2, 300])\n",
      "Y_train = np.zeros([num_train]) \n",
      "Y_test = np.zeros([num_test])\n",
      "\n",
      "# format data \n",
      "b = [a[None,:] for a in list(df['q1_feats'].values)]\n",
      "q1_feats = np.concatenate(b, axis=0)\n",
      "\n",
      "b = [a[None,:] for a in list(df['q2_feats'].values)]\n",
      "q2_feats = np.concatenate(b, axis=0)\n",
      "\n",
      "# fill data arrays with features\n",
      "X_train[:,0,:] = q1_feats[:num_train]\n",
      "X_train[:,1,:] = q2_feats[:num_train]\n",
      "Y_train = df[:num_train]['is_duplicate'].values\n",
      "            \n",
      "X_test[:,0,:] = q1_feats[num_train:]\n",
      "X_test[:,1,:] = q2_feats[num_train:]\n",
      "Y_test = df[num_train:]['is_duplicate'].values\n",
      "\n",
      "# remove useless variables\n",
      "del b\n",
      "del q1_feats\n",
      "del q2_feats\n",
      "\n",
      "# preprocess data, unit std\n",
      "#X_train_norm = np.zeros_like(X_train)\n",
      "#d = (np.sum(X_train[:,0,:] ** 2, 1) ** (0.5))\n",
      "#X_train_norm[:,0,:] = (X_train[:,0,:].T / (d + 1e-8)).T\n",
      "#d = (np.sum(X_train[:,1,:] ** 2, 1) ** (0.5))\n",
      "#X_train_norm[:,1,:] = (X_train[:,1,:].T / (d + 1e-8)).T\n",
      "#            \n",
      "#            \n",
      "#X_test_norm = np.zeros_like(X_test)\n",
      "#d = (np.sum(X_test[:,0,:] ** 2, 1) ** (0.5))\n",
      "#X_test_norm[:,0,:] = (X_test[:,0,:].T / (d + 1e-8)).T\n",
      "#d = (np.sum(X_test[:,1,:] ** 2, 1) ** (0.5))\n",
      "#X_test_norm[:,1,:] = (X_test[:,1,:].T / (d + 1e-8)).T\n",
      "\n",
      "##############################################################################\n",
      "# TRAIN MODEL\n",
      "# - 2 layers net : 0.67\n",
      "# - 3 layers net + adam : 0.74\n",
      "# - 3 layers resnet (after relu) + adam : 0.78\n",
      "# - 3 layers resnet (before relu) + adam : 0.77\n",
      "# - 3 layers resnet (before relu) + adam + dropout : 0.75\n",
      "# - 3 layers resnet (before relu) + adam + layer concat : 0.79\n",
      "# - 3 layers resnet (before relu) + adam + layer concat + unit_norm : 0.77\n",
      "# - 3 layers resnet (before relu) + adam + unit_norm + cosine_distance : Fail\n",
      "############################################################################## \n",
      "          \n",
      "# create model\n",
      "from siamese import *\n",
      "from keras.optimizers import RMSprop, SGD, Adam\n",
      "net = create_network(300)\n",
      "\n",
      "# train\n",
      "#optimizer = SGD(lr=1, momentum=0.8, nesterov=True, decay=0.004)\n",
      "optimizer = Adam(lr=0.001)\n",
      "net.compile(loss=contrastive_loss, optimizer=optimizer)\n",
      "\n",
      "for epoch in range(50):\n",
      "    net.fit([X_train_norm[:,0,:], X_train_norm[:,1,:]], Y_train,\n",
      "          validation_data=([X_test_norm[:,0,:], X_test_norm[:,1,:]], Y_test),\n",
      "          batch_size=128, nb_epoch=1, shuffle=True, )\n",
      "    \n",
      "    # compute final accuracy on training and test sets\n",
      "    pred = net.predict([X_test_norm[:,0,:], X_test_norm[:,1,:]], batch_size=128)\n",
      "    te_acc = compute_accuracy(pred, Y_test)\n",
      "    \n",
      "#    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
      "    print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import spacy\n",
      "nlp = spacy.load('en')\n",
      "\n",
      "vecs1 = []\n",
      "for qu in tqdm(list(df['question1'])):\n",
      "    doc = nlp(qu) \n",
      "    mean_vec = np.zeros([len(doc), 300])\n",
      "    for word in doc:\n",
      "        # word2vec\n",
      "        vec = word.vector\n",
      "        # fetch df score\n",
      "        try:\n",
      "            idf = word2tfidf[str(word)]\n",
      "        except:\n",
      "            #print word\n",
      "            idf = 0\n",
      "        # compute final vec\n",
      "        mean_vec += vec * idf\n",
      "    mean_vec = mean_vec.mean(axis=0)\n",
      "    vecs1.append(mean_vec)\n",
      "df['q1_feats'] = list(vecs1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "  0%|          | 0/404290 [00:00<?, ?it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "Word vectors set to length 0. This may be because the data is not installed. If you haven't already, run\npython -m spacy download en\nto install the data.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-16-138a6dee0ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# fetch df score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/spacy/tokens/token.so\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.vector.__get__ (spacy/tokens/token.cpp:7174)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Word vectors set to length 0. This may be because the data is not installed. If you haven't already, run\npython -m spacy download en\nto install the data."
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = nlp(list(df['question1'])[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "What is the step by step guide to invest in share market in india?"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_vec = np.zeros([len(doc), 300])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_vec.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "(15, 300)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}